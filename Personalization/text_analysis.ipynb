{
 "metadata": {
  "name": "",
  "signature": "sha256:e7982e2e3db47963a3ba8eb267c8efc9ec8cbff449afb837b635c0e69564df77"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "__author__ = 'mac'\n",
      "from os.path import join\n",
      "import csv\n",
      "\n",
      "PATH = \"/Users/mac/Dropbox/unicamp/TSI/trabajo-final/data\"\n",
      "NEWS_IC = \"noticias-ic-06-today.csv\"\n",
      "NEWS = \"noticias-unicamp.csv\"\n",
      "PALES = \"palestras.csv\"\n",
      "CDC = \"expos-cdc.csv\"\n",
      "LAGO = \"casa-do-lago.csv\"\n",
      "ANNOT = \"annotations.csv\"\n",
      "LABEL = \"labels.csv\"\n",
      "TAXONOMY = 'taxonomy.txt'\n",
      "TAXONOMY_LEN = 16\n",
      "BOW_CORPUS = \"bcorpus\"\n",
      "DICT = \"dict\"\n",
      "MODEL = \"model\"\n",
      "CLASSIFIER = \"classifier\"\n",
      "\n",
      "#def unicode_csv_reader(utf8_data, dialect=csv.excel, **kwargs):\n",
      "#    csv_reader = csv.reader(utf8_data, dialect=dialect, **kwargs)\n",
      "#    for row in csv_reader:\n",
      "#        yield [cell.encode('utf-8') for cell in row]\n",
      "\n",
      "def load_news():\n",
      "    reader = csv.reader(file(join(PATH, NEWS)), delimiter='$', quotechar='#')\n",
      "    res = []\n",
      "    for row in reader:\n",
      "        [date, title, content] = row\n",
      "        res.append(title + \"\\n\" + content)\n",
      "    return res\n",
      "\n",
      "def load_news_ic():\n",
      "    reader = csv.reader(file(join(PATH, NEWS_IC)), delimiter='$', quotechar='#')\n",
      "    res = []\n",
      "    for row in reader:\n",
      "        [title, content] = row\n",
      "        res.append(title + \"\\n\" + content)\n",
      "    return res\n",
      "\n",
      "def load_expo_cdc():\n",
      "    reader = csv.reader(file(join(PATH, CDC)), delimiter='$', quotechar='#')\n",
      "    res = []\n",
      "    for row in reader:\n",
      "        [inid, endd, inih, endh, title, content] = row\n",
      "        res.append(str(title + \"\\n\" + content))\n",
      "    return res\n",
      "\n",
      "def load_palestras():\n",
      "    reader = csv.reader(file(join(PATH, PALES)), delimiter='$', quotechar='#')\n",
      "    res = []\n",
      "    for row in reader:\n",
      "        [date, title, content] = row\n",
      "        res.append(title + \"\\n\" + content)\n",
      "    return res\n",
      "\n",
      "def load_lago():\n",
      "    reader = csv.reader(file(join(PATH, LAGO)), delimiter='$', quotechar='#')\n",
      "    res = []\n",
      "    for row in reader:\n",
      "        [ini, end, title, content] = row\n",
      "        res.append(title + \"\\n\" + content)\n",
      "    return res\n",
      "\n",
      "def load_labels():\n",
      "    reader = csv.reader(file(join(PATH, LABEL)), delimiter=',')\n",
      "    res = []\n",
      "    for row in reader:\n",
      "        for cell in row:\n",
      "            res.append(int(cell))\n",
      "    return res\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def load_annotations():\n",
      "    print \"load annotations\"\n",
      "    reader = csv.reader(file(join(PATH, ANNOT)), delimiter=',', quotechar='\"')\n",
      "    res = []\n",
      "    for row in reader:\n",
      "        res2 = []\n",
      "        for cell in row:\n",
      "            res2.append(cell)\n",
      "        res.append(res2)\n",
      "    res = np.array(res)\n",
      "    res = res[1:,:]\n",
      "    print res\n",
      "    for i in range(len(res)):\n",
      "        for j in range(len(res[0])):\n",
      "            res[i][j] = int(res[i][j])\n",
      "    return res\n",
      "\n",
      "\n",
      "def generate_word_cloud(corpora, outfile):\n",
      "    raise Exception(\"Implement this!\")\n",
      "\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import RSLPStemmer\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.corpus import stopwords\n",
      "import string\n",
      "\n",
      "def preprocessing(corpora):\n",
      "    stemmer = RSLPStemmer()\n",
      "    stemmer2 = PorterStemmer()\n",
      "    stp = stopwords.words('portuguese')\n",
      "    stp.append('\u00e9')\n",
      "    stp.append('ainda')\n",
      "    res = []\n",
      "    for i in range(len(corpora)):\n",
      "        corpora[i] = str(corpora[i]).lower()\n",
      "        corpora[i] = corpora[i].translate(None, string.punctuation)\n",
      "        corpora[i] = corpora[i].decode('utf-8')\n",
      "        corpora[i] = corpora[i].replace(u'\u201d',u'')\n",
      "        corpora[i] = corpora[i].replace(u'\u201c',u'')\n",
      "        corpora[i] = corpora[i].replace(u'\u2013',u'')\n",
      "        res2 = []\n",
      "        for t in word_tokenize(corpora[i]):\n",
      "            if t in stp:\n",
      "                continue\n",
      "            if(any(char.isdigit() for char in t)==False):\n",
      "                res2.append(stemmer2.stem(stemmer.stem(t)))\n",
      "        res.append(res2)\n",
      "    return res\n",
      "\n",
      "def preprocessing_wordcloud(corpora):\n",
      "    stp = stopwords.words('portuguese')\n",
      "    stp += stopwords.words('english')\n",
      "    stp += ['ainda', 'el', 'la', 'en', 'con', 'sob' ]\n",
      "    res = \"\"\n",
      "    for i in range(len(corpora)):\n",
      "        corpora[i] = corpora[i].lower()\n",
      "        corpora[i] = corpora[i].translate(None, string.punctuation)\n",
      "        corpora[i] = corpora[i].decode('utf-8')\n",
      "        corpora[i] = corpora[i].replace(u'\u201d',u'')\n",
      "        corpora[i] = corpora[i].replace(u'\u201c',u'')\n",
      "        corpora[i] = corpora[i].replace(u'\u2013',u'')\n",
      "        for t in word_tokenize(corpora[i]):\n",
      "            if t in stp:\n",
      "                continue\n",
      "            if(any(char.isdigit() for char in t)==False):\n",
      "                #print t.decode('utf-8')\n",
      "                res += \" \" + t\n",
      "    return res\n",
      "\n",
      "def save_wordclouds():\n",
      "    text = preprocessing_wordcloud(load_expo_cdc())\n",
      "    of = open(join(PATH, CDC + \"_cloud.txt\"), \"wb+\")\n",
      "    of.write(text.encode('utf-8'))\n",
      "    of.close()\n",
      "    text = preprocessing_wordcloud(load_lago())\n",
      "    of = open(join(PATH, LAGO + \"_cloud.txt\"), \"wb+\")\n",
      "    of.write(text.encode('utf-8'))\n",
      "    of.close()\n",
      "    text = preprocessing_wordcloud(load_news())\n",
      "    of = open(join(PATH, NEWS + \"_cloud.txt\"), \"wb+\")\n",
      "    of.write(text.encode('utf-8'))\n",
      "    of.close()\n",
      "    text = preprocessing_wordcloud(load_news_ic())\n",
      "    of = open(join(PATH, NEWS_IC + \"_cloud.txt\"), \"wb+\")\n",
      "    of.write(text.encode('utf-8'))\n",
      "    of.close()\n",
      "    text = preprocessing_wordcloud(load_palestras())\n",
      "    of = open(join(PATH, PALES + \"_cloud.txt\"), \"wb+\")\n",
      "    of.write(text.encode('utf-8'))\n",
      "    of.close()\n",
      "     \n",
      "from gensim import corpora, models, similarities\n",
      "from gensim.models.ldamodel import LdaModel\n",
      "import gensim \n",
      "\n",
      "class TextAnalyzer(object): \n",
      "    def __init__(self):\n",
      "        self.model = None\n",
      "        self.classifier = None\n",
      "        self.dictionary = None\n",
      "        self.corpus = None\n",
      "    \n",
      "    def load_taxonomy():\n",
      "        raise Exception(\"Implement this!\")\n",
      "        \n",
      "    def train(taxonomy, corpora, labels):\n",
      "        raise Exception(\"Implement this!\")\n",
      "\n",
      "    def predict(data):\n",
      "        raise Exception(\"Implement this!\")\n",
      "    \n",
      "        \n",
      "        \n",
      "\n",
      "#save_wordclouds()\n",
      "\n",
      "from sklearn.externals import joblib\n",
      "\n",
      "def generate_model():\n",
      "    np.set_printoptions(precision=2)\n",
      "    corpus = []\n",
      "    corpus += load_expo_cdc()\n",
      "    corpus += load_lago()\n",
      "    corpus += load_news()\n",
      "    corpus += load_news_ic()\n",
      "    corpus += load_palestras()\n",
      "    corpus = preprocessing(corpus)\n",
      "    dictionary = corpora.Dictionary(corpus)\n",
      "    bow_corpus = [dictionary.doc2bow(text) for text in corpus]\n",
      "    \n",
      "    dictionary.save(DICT)\n",
      "    corpora.MmCorpus.serialize(BOW_CORPUS, bow_corpus)\n",
      "    \n",
      "    bow2 = np.concatenate((bow_corpus, bow_corpus), axis=0)\n",
      "    bow2 = np.concatenate((bow2, bow2), axis=0)\n",
      "    bow2 = np.concatenate((bow2, bow2), axis=0)\n",
      "    TOPICS = 20\n",
      "    model = LdaModel(bow2, id2word=dictionary, num_topics=TOPICS, iterations=100, passes=15)\n",
      "    model.save(MODEL)\n",
      "    \n",
      "    lda_corpus = [model[vector] for vector in bow2]\n",
      "    lda_dense = gensim.matutils.corpus2dense(lda_corpus, num_terms=TOPICS).transpose()\n",
      "    \"\"\"\n",
      "    tfidf = models.TfidfModel(bow_corpus)\n",
      "    tfidf_corpus = [tfidf[vector] for vector in bow_corpus]\n",
      "    tfidf_dense = gensim.matutils.corpus2dense(tfidf_corpus, num_terms=len(dictionary)).transpose()\n",
      "    \"\"\"\n",
      "    classifier = LogisticRegression()\n",
      "    labels = load_labels()\n",
      "    labels2 = labels\n",
      "    labels2 += labels2\n",
      "    labels2 += labels2\n",
      "    labels2 += labels2\n",
      "    classifier.fit(lda_dense, labels2)\n",
      "    joblib.dump(classifier, CLASSIFIER, compress=9)\n",
      "    print \"LDA results\"\n",
      "    probs = classifier.predict_proba(lda_dense)\n",
      "    print probs\n",
      "    \n",
      "class TextAnalyzer(object):\n",
      "    def read_model(self):\n",
      "        self.dictionary = corpora.Dictionary.load(DICT)\n",
      "        self.bow_corpus = corpora.MmCorpus(BOW_CORPUS)\n",
      "        self.lda_model = LdaModel.load(MODEL)\n",
      "        self.logit_classifier = joblib.load(CLASSIFIER)\n",
      "\n",
      "        corpus = []\n",
      "        corpus += load_expo_cdc()\n",
      "        corpus += load_lago()\n",
      "        corpus += load_news()\n",
      "        corpus += load_news_ic()\n",
      "        corpus += load_palestras()\n",
      "        corpus = preprocessing(corpus)\n",
      "\n",
      "        test_bow = [self.dictionary.doc2bow(text) for text in corpus]\n",
      "        lda_corpus = [self.lda_model[bow] for bow in test_bow]\n",
      "        lda_dense = gensim.matutils.corpus2dense(lda_corpus, num_terms=TOPICS).transpose()\n",
      "        probs = self.logit_classifier.predict_proba(lda_dense)\n",
      "        #print \"RESULTS IN READED MODEL\"\n",
      "        #print probs\n",
      "    def predict(self, text):\n",
      "        text = [text]\n",
      "        text = preprocessing(text)\n",
      "        print text\n",
      "        test_bow = [self.dictionary.doc2bow(row) for row in text]\n",
      "        lda_corpus = [self.lda_model[bow] for bow in test_bow]\n",
      "        lda_dense = gensim.matutils.corpus2dense(lda_corpus, num_terms=TOPICS).transpose()\n",
      "        probs = self.logit_classifier.predict_proba(lda_dense)\n",
      "        return probs[0]\n",
      "\n",
      "def default_profile():\n",
      "    return [0.5]*TAXONOMY_LEN\n",
      "    \n",
      "def update_profile_by_data(profile, analyzer, data, delta = 0.1):\n",
      "    labels = analyzer.predict(data)\n",
      "    max_ = max(labels)\n",
      "    min_ = min(labels)\n",
      "    for i in range(len(labels)):\n",
      "        labels[i] = (labels[i]-min_)/(max_-min_)\n",
      "    assert len(labels) == len(profile)\n",
      "    for i in range(len(profile)):\n",
      "        profile[i] = profile[i] * 0.9 +  labels[i] * 0.1\n",
      "    return profile;\n",
      "\n",
      "def testAll():\n",
      "    analyzer = TextAnalyzer()\n",
      "    analyzer.read_model()\n",
      "    corpus = []\n",
      "    corpus += load_expo_cdc()\n",
      "    corpus += load_lago()\n",
      "    corpus += load_news()\n",
      "    corpus += load_news_ic()\n",
      "    corpus += load_palestras()\n",
      "\n",
      "    profile = default_profile();\n",
      "    print profile\n",
      "    profile = update_profile_by_data(profile, analyzer, corpus[0])\n",
      "    print profile\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
        "[[u'expo', u'rhinomorf', u'laur', u'garay', u'artist', u'pl\\xe1', u'laur', u'garay', u'formand', u'institut', u'art', u'unicamp', u'junt', u'coordenad', u'desenvolv', u'cult', u'unicamp', u'convid', u'tod', u'expo', u'rhinomorf', u's\\xe9ri', u'threre\\xb4', u'rhin', u'in', u'the', u'room', u'result', u'trabalh', u'experi', u'art', u'visual', u'sob', u'orient', u'prof', u'dra', u'sylv', u'furegatt', u'mostr', u'acontec', u'centr', u'conven\\xe7', u'unicamp', u'abert', u'dia', u'dezembr', u'visit', u'acontec', u'dezembr', u'inform', u'contat', u'email', u'projartunicampbr']]\n",
        "[0.46205697240807453, 0.45651054137319452, 0.45066169445846177, 0.55000000000000004, 0.48802424013672396, 0.45616284486657754, 0.50646754721017773, 0.4506282469666239, 0.46208293769706427, 0.45000000000000001, 0.47102801973065828, 0.49779641095798771, 0.4761640745501281, 0.45916797830201417, 0.5449698093921187, 0.45019762260811641]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "'\\nclassifier = LogisticRegression()\\nlabels = load_labels()\\nprint \"TFIDF results\"\\nclassifier.fit(tfidf_dense, labels)\\nprobs = classifier.predict_proba(tfidf_dense)\\nprint probs\\n'"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}